{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73f2366-7afc-4ce5-a2ef-ada192a54373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### READ JSON DATA (BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3671159a-14c0-412a-8264-e41b6080ea6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae737c1a-9adb-4475-8867-416db910a3de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### READ JSON DATA (BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e31468-4b64-47be-8e0b-5d7a7b80b531",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759527937117}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.format(\"json\")\\\n",
    "#             .option(\"inferScehma\", True)\\\n",
    "#             .option(\"multiline\", True)\\\n",
    "#             .load(\"/Volumes/workspace/stream/streaming/jsonsource\")\n",
    "# #flatten order_id and timestamp\n",
    "# df = df.select(\"order_id\", \"timestamp\", \"customer\", \"items\", \"payment\", \"metadata\")\n",
    "\n",
    "# #flatten customer\n",
    "# df = df.select(\"order_id\", \"timestamp\", \"customer.customer_id\", \"customer.name\", \"customer.email\", \"customer.address.city\", \"customer.address.country\", \"customer.address.postal_code\", \"items\", \"payment\", \"metadata\")\n",
    "\n",
    "# #explode items list\n",
    "# df = df.withColumn(\"items\", explode_outer(\"items\"))\n",
    "\n",
    "# #flatten items\n",
    "# df = df.select(\"items.item_id\", \"items.price\", \"items.product_name\", \"items.quantity\", \"order_id\", \"timestamp\", \"customer_id\", \"name\", \"email\", \"city\", \"country\", \"postal_code\", \"payment\", \"metadata\")\n",
    "\n",
    "# #flatten payment\n",
    "# df = df.select(\"order_id\", \"timestamp\", \"item_id\", \"price\", \"product_name\", \"quantity\", \"order_id\", \"timestamp\", \"customer_id\", \"name\", \"email\", \"city\", \"country\", \"postal_code\", \"payment.method\", \"payment.transaction_id\", \"metadata\")\n",
    "\n",
    "# #explode metadata list\n",
    "# df = df.withColumn(\"metadata\", explode_outer(\"metadata\"))\n",
    "\n",
    "# #flatten metadata\n",
    "# df = df.select(\"*\", \"metadata.key\", \"metadata.value\").drop(\"metadata\")\n",
    "\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066867ab-7aa7-43a7-a301-4222ca7e6974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### READ STREAMING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516871ad-d2fd-40ab-b089-d463ee172543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = \"\"\"order_id STRING,\n",
    "timestamp STRING,\n",
    "customer STRUCT<\n",
    "  customer_id: INT,\n",
    "  name: STRING,\n",
    "  email: STRING,\n",
    "  address: STRUCT<\n",
    "    city: STRING,\n",
    "    postal_code: STRING,\n",
    "    country: STRING\n",
    "  >\n",
    ">,\n",
    "items ARRAY<STRUCT<\n",
    "  item_id: STRING,\n",
    "  product_name: STRING,\n",
    "  quantity: INT,\n",
    "  price: DOUBLE\n",
    ">>,\n",
    "payment STRUCT<\n",
    "  method: STRING,\n",
    "  transaction_id: STRING\n",
    ">,\n",
    "metadata ARRAY<STRUCT<\n",
    "  key: STRING,\n",
    "  value: STRING\n",
    ">>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0fa9ea-2c98-4e5c-ba1d-916f1d1d554d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### FLATTEN JSON\n",
    "get rid of hierarchy in json to make data easier to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f375438-850d-4dda-8836-28efd0bf6a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### EXPLODE ARRAY\n",
    "convert every item in array as a row\n",
    "explode() - removes NULL values. Drops entire row for one NULL\n",
    "explode_outer() - explode all values, including NULL. You want this so a row won't be dropped for one NULL in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021f4a07-9770-4d82-9b6a-ea06f716dfc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer, col\n",
    "\n",
    "# Streaming query\n",
    "df = spark.readStream.format(\"json\")\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .schema(my_schema)\\\n",
    "    .load(\"/Volumes/workspace/stream/streaming/jsonsource\")\n",
    "\n",
    "# Flatten customer\n",
    "df = df.select(\n",
    "    \"order_id\", \"timestamp\",\n",
    "    col(\"customer.customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer.name\").alias(\"customer_name\"),\n",
    "    col(\"customer.email\").alias(\"customer_email\"),\n",
    "    col(\"customer.address.city\").alias(\"customer_city\"),\n",
    "    col(\"customer.address.country\").alias(\"customer_country\"),\n",
    "    col(\"customer.address.postal_code\").alias(\"customer_postal_code\"),\n",
    "    \"items\", \"payment\", \"metadata\"\n",
    ")\n",
    "\n",
    "# Explode items list\n",
    "df = df.withColumn(\"item\", explode_outer(\"items\"))\n",
    "\n",
    "# Flatten items\n",
    "df = df.select(\n",
    "    \"order_id\", \"timestamp\", \"customer_id\", \"customer_name\", \"customer_email\",\n",
    "    \"customer_city\", \"customer_country\", \"customer_postal_code\",\n",
    "    col(\"item.item_id\").alias(\"item_id\"),\n",
    "    col(\"item.price\").alias(\"item_price\"),\n",
    "    col(\"item.product_name\").alias(\"item_product_name\"),\n",
    "    col(\"item.quantity\").alias(\"item_quantity\"),\n",
    "    \"payment\", \"metadata\"\n",
    ")\n",
    "\n",
    "# Flatten payment\n",
    "df = df.select(\n",
    "    \"order_id\", \"timestamp\", \"customer_id\", \"customer_name\", \"customer_email\",\n",
    "    \"customer_city\", \"customer_country\", \"customer_postal_code\",\n",
    "    \"item_id\", \"item_price\", \"item_product_name\", \"item_quantity\",\n",
    "    col(\"payment.method\").alias(\"payment_method\"),\n",
    "    col(\"payment.transaction_id\").alias(\"payment_transaction_id\"),\n",
    "    \"metadata\"\n",
    ")\n",
    "\n",
    "# Explode metadata list\n",
    "df = df.withColumn(\"metadata_item\", explode_outer(\"metadata\"))\n",
    "\n",
    "# Flatten metadata\n",
    "df = df.select(\n",
    "    \"order_id\", \"timestamp\", \"customer_id\", \"customer_name\", \"customer_email\",\n",
    "    \"customer_city\", \"customer_country\", \"customer_postal_code\",\n",
    "    \"item_id\", \"item_price\", \"item_product_name\", \"item_quantity\",\n",
    "    \"payment_method\", \"payment_transaction_id\",\n",
    "    col(\"metadata_item.key\").alias(\"metadata_key\"),\n",
    "    col(\"metadata_item.value\").alias(\"metadata_value\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36798760-7f55-4ac9-a729-1f9490ebc676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.writeStream.format(\"delta\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .trigger(once=True)\\\n",
    "        .option(\"path\", \"/Volumes/workspace/stream/streaming/jsonsink/Data\")\\\n",
    "        .option(\"checkpointLocation\", \"/Volumes/workspace/stream/streaming/jsonsink/checkpoint\")\\\n",
    "        .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8cd69f8-e780-433b-b376-9916e3955dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Cleaner\n",
    "# dbutils.fs.rm(\"/Volumes/workspace/stream/streaming/jsonsink/Data\", recurse=True)\n",
    "# dbutils.fs.rm(\"/Volumes/workspace/stream/streaming/jsonsink/checkpoint\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0421a7e-0ad9-42d8-982f-c2ee4cac98f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    ".trigger(once=True)\\\n",
    "Usually triggers every amount of period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04249d1b-0d81-48e8-b5f5-bd111fa0ee49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM delta.`/Volumes/workspace/stream/streaming/jsonsink/Data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77ce3e00-7470-4c50-91e2-f662526f3bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Idempotency\n",
    "Spark will only process new information.\n",
    "Checkpoint saves what has been processed so we don't have to do it again.\n",
    "\n",
    "### how does it work?\n",
    "- each query is assigned an id\n",
    "- when a query finishes pretend it's being interrupted\n",
    "- when another query is executed it will start off where the previous id query was interrupted\n",
    "\n",
    "### commits folder in checkpoint\n",
    "it tracks all the files\n",
    "(doesn't work for delta files as a source)\n",
    "- this folder remembers which files have been read\n",
    "- it remembers names, so deleting and reuploading the same file or uploading a dupe will still be ignored when processing\n",
    "- it only checks if the whole file is read\n",
    "- **NEVER TOUCH THIS. managed by spark and if you touch it it will break**\n",
    "\n",
    "### triggers\n",
    "#### default mode\n",
    "imagine a micro batch\n",
    "the next micro batch will be processed once the current one is finished\n",
    "\n",
    "#### processing time\n",
    "```\n",
    ".trigger(processingTime=\"10 seconds\")\n",
    "```\n",
    "when the source receives any data a timer starts for it to check if anything new came in.\n",
    "\n",
    "In this case, it will check the source for new data, then start processing, then start the 10 second timer, once it hits 10 seconds, it checks the source folder again and repeats this process from the beginning.\n",
    "\n",
    "#### Once\n",
    "load all the data and process all together\n",
    "basically batch processing\n",
    "good when starting out and testing\n",
    "\n",
    "#### Available Now\n",
    "load all the data, in one MB(micro batch). \n",
    "Divides into a lot of smaller microbatches to stream it into the process\n",
    "better than ONCE\n",
    "\n",
    "#### Continuous !!\n",
    "most recently added to spark\n",
    "only supports append only.\n",
    "\n",
    "process data row by row\n",
    "```\n",
    ".trigger(continuous='1 second')\n",
    "```\n",
    "the period of time is for checkpoint Location. It does not stand for processing time.\n",
    "\n",
    "### archiving source files\n",
    "To keep the source folder clean, processed files are moved to Archive directory.\n",
    "Only files that are processed go into archives. So, duplicates will not end up there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b007e6a5-8752-499e-9952-0267f3787204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Archiving example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d949a8da-bde6-4d6f-9177-bb725e9d7a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer, col\n",
    "\n",
    "# Streaming query\n",
    "df = spark.readStream.format(\"json\")\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .schema(my_schema)\\\n",
    "    .option(\"cleanSource\", \"archive\")\\\n",
    "    .option(\"sourceArchiveDir\", \"/Volumes/workspace/stream/streaming/jsonsourcearchive\")\\\n",
    "    .load(\"/Volumes/workspace/stream/streaming/jsonsourcenew\")\n",
    "\n",
    "# Flatten customer\n",
    "df = df.select(\n",
    "    \"order_id\", \"timestamp\",\n",
    "    col(\"customer.customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer.name\").alias(\"customer_name\"),\n",
    "    col(\"customer.email\").alias(\"customer_email\"),\n",
    "    col(\"customer.address.city\").alias(\"customer_city\"),\n",
    "    col(\"customer.address.country\").alias(\"customer_country\"),\n",
    "    col(\"customer.address.postal_code\").alias(\"customer_postal_code\"),\n",
    "    \"items\", \"payment\", \"metadata\"\n",
    ")\n",
    "\n",
    "# Explode items list\n",
    "df = df.withColumn(\"item\", explode_outer(\"items\"))\n",
    "\n",
    "# Flatten items\n",
    "df = df.select(\n",
    "    \"order_id\", \"timestamp\", \"customer_id\", \"customer_name\", \"customer_email\",\n",
    "    \"customer_city\", \"customer_country\", \"customer_postal_code\",\n",
    "    col(\"item.item_id\").alias(\"item_id\"),\n",
    "    col(\"item.price\").alias(\"item_price\"),\n",
    "    col(\"item.product_name\").alias(\"item_product_name\"),\n",
    "    col(\"item.quantity\").alias(\"item_quantity\"),\n",
    "    \"payment\", \"metadata\"\n",
    ")\n",
    "\n",
    "# Flatten payment\n",
    "df = df.select(\n",
    "    \"order_id\", \"timestamp\", \"customer_id\", \"customer_name\", \"customer_email\",\n",
    "    \"customer_city\", \"customer_country\", \"customer_postal_code\",\n",
    "    \"item_id\", \"item_price\", \"item_product_name\", \"item_quantity\",\n",
    "    col(\"payment.method\").alias(\"payment_method\"),\n",
    "    col(\"payment.transaction_id\").alias(\"payment_transaction_id\"),\n",
    "    \"metadata\"\n",
    ")\n",
    "\n",
    "# Explode metadata list\n",
    "df = df.withColumn(\"metadata_item\", explode_outer(\"metadata\"))\n",
    "\n",
    "# Flatten metadata\n",
    "df = df.select(\n",
    "    \"order_id\", \"timestamp\", \"customer_id\", \"customer_name\", \"customer_email\",\n",
    "    \"customer_city\", \"customer_country\", \"customer_postal_code\",\n",
    "    \"item_id\", \"item_price\", \"item_product_name\", \"item_quantity\",\n",
    "    \"payment_method\", \"payment_transaction_id\",\n",
    "    col(\"metadata_item.key\").alias(\"metadata_key\"),\n",
    "    col(\"metadata_item.value\").alias(\"metadata_value\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c10e9a5-b965-4dae-aa5b-52b65ded0566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.writeStream.format(\"delta\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .trigger(once=True)\\\n",
    "        .option(\"path\", \"/Volumes/workspace/stream/streaming/jsonsinknew/Data\")\\\n",
    "        .option(\"checkpointLocation\", \"/Volumes/workspace/stream/streaming/jsonsinknew/checkpoint\")\\\n",
    "        .start()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5892509575538200,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "stream tutorial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
